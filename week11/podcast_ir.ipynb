{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333ae546d607a744",
   "metadata": {},
   "source": [
    "# Workshop: Building an Information Retrieval System for Podcast Episodes\n",
    "\n",
    "## Objective:\n",
    "Create an Information Retrieval (IR) system that processes a dataset of podcast transcripts and, given a query, returns the episodes where the host and guest discuss the query topic. Use TF-IDF and BERT for vector space representation and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a934919d95ac2de",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "### Step 1: Import Libraries\n",
    "Import necessary libraries for data handling, text processing, and machine learning.\n",
    "\n",
    "### Step 2: Load the Dataset\n",
    "\n",
    "Load the dataset of podcast transcripts.\n",
    "\n",
    "Find the dataset in: https://www.kaggle.com/datasets/rajneesh231/lex-fridman-podcast-transcript\n",
    "\n",
    "### Step 3: Text Preprocessing\n",
    "\n",
    "You know what to do ;)\n",
    "\n",
    "###  Step 4: Vector Space Representation - TF-IDF\n",
    "\n",
    "Create TF-IDF vector representations of the transcripts.\n",
    "\n",
    "### Step 5: Vector Space Representation - BERT\n",
    "\n",
    "Create BERT vector representations of the transcripts using a pre-trained BERT model.\n",
    "\n",
    "### Step 6: Query Processing\n",
    "\n",
    "Define a function to process the query and compute similarity scores using both TF-IDF and BERT embeddings.\n",
    "\n",
    "### Step 7: Retrieve and Compare Results\n",
    "\n",
    "Define a function to retrieve the top results based on similarity scores for both TF-IDF and BERT representations.\n",
    "\n",
    "### Step 8: Test the IR System\n",
    "\n",
    "Test the system with a sample query.\n",
    "\n",
    "Retrieve and display the top results using both TF-IDF and BERT representations.\n",
    "\n",
    "### Step 9: Compare Results\n",
    "\n",
    "Analyze and compare the results obtained from TF-IDF and BERT representations.\n",
    "\n",
    "Discuss the differences, strengths, and weaknesses of each method based on the retrieval results.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "* Follow the steps outlined above to implement the IR system.\n",
    "* Run the provided code snippets to understand how each part of the system works.\n",
    "* Test the system with various queries to observe the results from both TF-IDF and BERT representations.\n",
    "* Compare and analyze the results. Discuss the pros and cons of each method.\n",
    "* Document your findings and any improvements you make to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-12 12:03:03.803922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-12 12:03:03.823767: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-12 12:03:03.823799: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-12 12:03:03.836593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-12 12:03:05.296055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "### Step 1: Import Libraries\n",
    "#Import necessary libraries for data handling, text processing, and machine learning.\n",
    "                          \n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5494db2-cba7-474d-8e9f-3e3b1f2881e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id            guest                    title  \\\n",
      "0   1      Max Tegmark                 Life 3.0   \n",
      "1   2    Christof Koch            Consciousness   \n",
      "2   3    Steven Pinker  AI in the Age of Reason   \n",
      "3   4    Yoshua Bengio            Deep Learning   \n",
      "4   5  Vladimir Vapnik     Statistical Learning   \n",
      "\n",
      "                                                text  \n",
      "0  As part of MIT course 6S099, Artificial Genera...  \n",
      "1  As part of MIT course 6S099 on artificial gene...  \n",
      "2  You've studied the human mind, cognition, lang...  \n",
      "3  What difference between biological neural netw...  \n",
      "4  The following is a conversation with Vladimir ...  \n",
      "0      As part of MIT course 6S099, Artificial Genera...\n",
      "1      As part of MIT course 6S099 on artificial gene...\n",
      "2      You've studied the human mind, cognition, lang...\n",
      "3      What difference between biological neural netw...\n",
      "4      The following is a conversation with Vladimir ...\n",
      "                             ...                        \n",
      "314    By the time he gets to 2045, we'll be able to ...\n",
      "315    there's a broader question here, right? As we ...\n",
      "316    Once this whole thing falls apart and we are c...\n",
      "317    you could be the seventh best player in the wh...\n",
      "318    turns out that if you train a planarian and th...\n",
      "Name: text, Length: 319, dtype: object\n"
     ]
    }
   ],
   "source": [
    "### Step 2: Load the Dataset\n",
    "\n",
    "#Load the dataset of podcast transcripts.\n",
    "\n",
    "#Find the dataset in: https://www.kaggle.com/datasets/rajneesh231/lex-fridman-podcast-transcript\n",
    "\n",
    "podcast_df = pd.read_csv('data/podcastdata_dataset.csv')\n",
    "print(podcast_df.head())\n",
    "corpus = podcast_df['text']\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89442b60-fcad-4f9a-9966-19b192ee5ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removal and file saving completed.\n"
     ]
    }
   ],
   "source": [
    "### Step 3: Text Preprocessing\n",
    "\n",
    "#You know what to do ;)\n",
    "stop_words_file = 'reuters/stopwords.txt'\n",
    "with open(stop_words_file, 'r', encoding='utf-8') as file:\n",
    "    stop_words = set(file.read().split())\n",
    "\n",
    "#print(stop_words)\n",
    "\n",
    "def clean_text(corpus):\n",
    "    cln_txt = []\n",
    "    for txt in corpus:\n",
    "        content = txt.lower()\n",
    "        # Removemos las stop words\n",
    "        cleaned_content = ' '.join([word for word in content.split() if word not in stop_words])\n",
    "        cln_txt.append(cleaned_content)\n",
    "    return cln_txt\n",
    "\n",
    "corpus_clean = clean_text(corpus)\n",
    "#print(corpus_clean[0])\n",
    "        \n",
    "print(\"Stop words removal and file saving completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85112533-f6a8-4fe9-a919-df4eae5246ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been processed and cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Funcion para limpiar caracteres especiales\n",
    "def clean_special_chars(corpus):\n",
    "    clean_corpus = []\n",
    "    for txt in corpus_clean:\n",
    "        cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', txt)\n",
    "        clean_corpus.append(cleaned_text)\n",
    "    return clean_corpus\n",
    "\n",
    "corpus_char_clean = clean_special_chars(corpus_clean)\n",
    "\n",
    "\n",
    "print(\"All files have been processed and cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bfa2bde-f5f5-41d2-8010-6cd553347396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo de lenguaje de spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Inicializar el stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Funci√≥n para lematizar y stematizar texto\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    stemmed_tokens = [stemmer.stem(token.text) for token in doc]\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(stemmed_tokens), ' '.join(lemmatized_tokens)\n",
    "\n",
    "def lema_stema_corpus(corpus):\n",
    "    lematizedCorpus=[]\n",
    "    stematizedCorpus=[]\n",
    "    \n",
    "    for txt in corpus:\n",
    "        stemmed_text, lemmatized_text = preprocess_text(txt)\n",
    "        lematizedCorpus.append(lemmatized_text)\n",
    "        stematizedCorpus.append(stemmed_text)\n",
    "    return lematizedCorpus, stematizedCorpus\n",
    "\n",
    "myLemaCorpus, myStemaCorpus = lema_stema_corpus(corpus_char_clean)\n",
    "print(\"Procesamiento completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43db9e4b-2e20-41c9-a3ef-a525fc64b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Step 4: Vector Space Representation - TF-IDF\n",
    "\n",
    "#Create TF-IDF vector representations of the transcripts.\n",
    "# Funci√≥n para vectorizar textos utilizando TF-IDF\n",
    "def TF_IDF(texts):\n",
    "    # Vectorizaci√≥n usando TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    return X_tfidf, tfidf_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6544a1ba-614f-4253-8cc4-d0ee90325b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Lemmatized:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Caracteristicas de TF-IDF Lemmatized: ['00' '000' '00000' ... 'zyklon' 'zz' 'zzzt']\n"
     ]
    }
   ],
   "source": [
    "# Utilizando corpus Lemmatized\n",
    "X_tfidf_lemmatized, tfidf_vectorizer_lemmatized = TF_IDF(myLemaCorpus)\n",
    "\n",
    "# Ver los resultados de TF-IDF\n",
    "print(\"TF-IDF Lemmatized:\")\n",
    "print(X_tfidf_lemmatized.toarray())\n",
    "print(\"Caracteristicas de TF-IDF Lemmatized:\", tfidf_vectorizer_lemmatized.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fd20b3-a872-4ea5-afe7-d6908e5f44f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Stemmed:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Caracteristicas de TF-IDF Stemmed: ['00' '000' '00000' ... 'zyklon' 'zz' 'zzzt']\n"
     ]
    }
   ],
   "source": [
    "#Utilizando corpus stemmed\n",
    "# Vectorizar los documentos\n",
    "X_tfidf_Stemmed, tfidf_vectorizer_Stemmed = TF_IDF(myStemaCorpus)\n",
    "\n",
    "# Ver los resultados de TF-IDF\n",
    "print(\"TF-IDF Stemmed:\")\n",
    "print(X_tfidf_Stemmed.toarray())\n",
    "print(\"Caracteristicas de TF-IDF Stemmed:\", tfidf_vectorizer_Stemmed.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b588152-2f1c-4296-8286-520ad64e7678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus shape:  (319,)\n",
      "BERT Embeddings: (319, 768)\n"
     ]
    }
   ],
   "source": [
    "### Step 5: Vector Space Representation - BERT\n",
    "#Create BERT vector representations of the transcripts using a pre-trained BERT model.\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#serial approach\n",
    "#def generate_bert_embeddings(texts):\n",
    "#    embeddings = []\n",
    "#    for text in texts:\n",
    "#        inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "#        outputs = model(**inputs)\n",
    "#        embeddings.append(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "#    return np.array(embeddings).transpose(0,2,1)\n",
    "\n",
    "def generate_bert_embeddings(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :].numpy())  # Use [CLS] token representation\n",
    "    return np.vstack(embeddings)  # Stack embeddings into a 2D array\n",
    "\n",
    "bert_embeddings = generate_bert_embeddings(corpus)\n",
    "print(\"corpus shape: \", corpus.shape)\n",
    "print(\"BERT Embeddings:\", bert_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e41573c-4d36-49fb-8d36-fe37b14d9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 6: Query Processing\n",
    "\n",
    "#Define a function to process the query and compute similarity scores using both TF-IDF and BERT embeddings.\n",
    "\n",
    "# TF-IDF query cleaning\n",
    "def clean_query(query):\n",
    "    # Limpiamos la query\n",
    "    query = query.lower()\n",
    "    stop_words_file = 'reuters/stopwords.txt'\n",
    "    with open(stop_words_file, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(file.read().split())\n",
    "\n",
    "    cleaned_query = ' '.join([word for word in query.split() if word not in stop_words])\n",
    "    cleaned_query = re.sub(r'[^A-Za-z0-9\\s]', '', cleaned_query)\n",
    "    return cleaned_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0927eca-936a-41cc-9083-6b2dc90ca77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 7: Retrieve and Compare Results\n",
    "\n",
    "#Define a function to retrieve the top results based on similarity scores for both TF-IDF and BERT representations.\n",
    "\n",
    "def top_n_documents(scores, n, corpus):\n",
    "\n",
    "    # Asegurarse de que los puntajes sean un array 1D\n",
    "    scores = scores.flatten()\n",
    "\n",
    "    # Obtener los √≠ndices de los n puntajes m√°s altos\n",
    "    top_indices = np.argsort(scores)[-n:][::-1]\n",
    "\n",
    "    # Obtener los textos correspondientes a esos √≠ndices\n",
    "\n",
    "    top_texts = []\n",
    "    top_titles = []\n",
    "\n",
    "    for idx in top_indices:\n",
    "        top_titles.append(corpus['title'][idx])\n",
    "        top_texts.append(corpus['text'][idx])\n",
    "\n",
    "    top_scores = scores[top_indices]\n",
    "\n",
    "    # Imprimir los 10 textos con mayor similitud y sus puntajes\n",
    "    for i, (text, title, score) in enumerate(zip(top_texts, top_titles, top_scores), 1):\n",
    "\n",
    "        print(f\"Top {i}: {title} - Similitud: {score:.4f}\")\n",
    "        print(text[19:75])\n",
    "        print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b25ae936-bffb-46da-a1c9-862b4134e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (1, 768)\n",
      "Document 273 - Bitcoin, Inflation, and the Future of Money with similarity score: 0.757319\n",
      "Transcript: hington? You know how he died? Well meaning physicians bled him to death, and this was the most important patient in the country, m\n",
      "\n",
      "Document 199 - Totalitarianism and Anarchy with similarity score: 0.754713\n",
      "Transcript: conversation between me and Michael Malus. Michael is an author, anarchist, and simpleton, and I'm proud to call him my friend. He \n",
      "\n",
      "Document 165 - Deep Work, Focus, Productivity, Email, and Social Media with similarity score: 0.744074\n",
      "Transcript: conversation with Cal Newport. He's a friend and someone who's writing, like his book, Deep Work, for example, has guided how I str\n",
      "\n",
      "Document 96 - Going Big in Business, Investing, and AI with similarity score: 0.737252\n",
      "Transcript: conversation with Stephen Schwarzman, CEO and cofounder of Blackstone, one of the world's leading investment firms with over $530 b\n",
      "\n",
      "Document 153 - Aliens, Black Holes, and the Mystery of the Oumuamua with similarity score: 0.734598\n",
      "Transcript: conversation with Avi Loeb, an astrophysicist, astronomer, and cosmologist at Harvard. He has authored over 800 papers and written \n",
      "\n",
      "--------------------------------------------\n",
      "Top 1: Moore‚Äôs Law, Microprocessors, Abstractions, and First Principles - Similitud: 0.1185\n",
      "conversation with Jim Keller, legendary microprocessor e\n",
      "\n",
      "Top 2: Self-Driving Cars at Aurora, Google, CMU, and DARPA - Similitud: 0.0858\n",
      "conversation with Chris Sampson. He was a CTO of the Goo\n",
      "\n",
      "Top 3: Qualcomm CEO - Similitud: 0.0857\n",
      "citing thing for an engineer, the same Snapdragon that g\n",
      "\n",
      "Top 4: Google - Similitud: 0.0801\n",
      "conversation with Eric Schmidt. He was the CEO of Google\n",
      "\n",
      "Top 5: Flying Cars, Autonomous Vehicles, and Education - Similitud: 0.0769\n",
      "conversation with Sebastian Thrun. He's one of the great\n",
      "\n",
      "Top 6: Affective Computing, Emotion, Privacy, and Health - Similitud: 0.0725\n",
      "conversation with Rosalind Picard. She's a professor at \n",
      "\n",
      "Top 7: Aliens, Technology, Religion, and the Nature of Belief - Similitud: 0.0689\n",
      "conversation with Diana Walsh Basolka, a professor of ph\n",
      "\n",
      "Top 8: Computer Vision - Similitud: 0.0638\n",
      "conversation with Jitendra Malik, a professor at Berkele\n",
      "\n",
      "Top 9: Quantum Computing - Similitud: 0.0606\n",
      "conversation with Scott Aaronson, a professor at UT Aust\n",
      "\n",
      "Top 10: Computer Architecture and Data Storage - Similitud: 0.0585\n",
      "conversation with David Patterson, touring award winner \n",
      "\n",
      "--------------------------------------------\n",
      "Top 1: Moore‚Äôs Law, Microprocessors, Abstractions, and First Principles - Similitud: 0.1702\n",
      "conversation with Jim Keller, legendary microprocessor e\n",
      "\n",
      "Top 2: Cellular Automata, Computation, and Physics - Similitud: 0.1137\n",
      "conversation with Stephen Wolfram, a computer scientist,\n",
      "\n",
      "Top 3: Qualcomm CEO - Similitud: 0.1067\n",
      "citing thing for an engineer, the same Snapdragon that g\n",
      "\n",
      "Top 4: Simulation and Superintelligence - Similitud: 0.0968\n",
      "conversation with Nick Bostrom, a philosopher at Univers\n",
      "\n",
      "Top 5: Self-Driving Cars at Aurora, Google, CMU, and DARPA - Similitud: 0.0906\n",
      "conversation with Chris Sampson. He was a CTO of the Goo\n",
      "\n",
      "Top 6: Robotics - Similitud: 0.0891\n",
      "conversation with Rodney Brooks, one of the greatest rob\n",
      "\n",
      "Top 7: Flying Cars, Autonomous Vehicles, and Education - Similitud: 0.0875\n",
      "conversation with Sebastian Thrun. He's one of the great\n",
      "\n",
      "Top 8: Affective Computing, Emotion, Privacy, and Health - Similitud: 0.0863\n",
      "conversation with Rosalind Picard. She's a professor at \n",
      "\n",
      "Top 9: Quantum Computing - Similitud: 0.0856\n",
      "conversation with Scott Aaronson, a professor at UT Aust\n",
      "\n",
      "Top 10: Google - Similitud: 0.0846\n",
      "conversation with Eric Schmidt. He was the CEO of Google\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Step 8: Test the IR System\n",
    "\n",
    "# Test the system with a sample query.\n",
    "\n",
    "# Retrieve and display the top results using both TF-IDF and BERT representations.\n",
    "\n",
    "# Input you query text\n",
    "# userInput = input(\"escribe tu frase de busqueda: \")\n",
    "userInput = \"technology computer innovation tomorrow new\"\n",
    "\n",
    "# Generate embeddings for the input\n",
    "## TF-IDF\n",
    "cleaned_query = clean_query(userInput)\n",
    "# lematizamos y stematizamos la query\n",
    "stemmed_query, lemmatized_query = preprocess_text(cleaned_query)\n",
    "\n",
    "## BERT\n",
    "input_bert_embeddings = generate_bert_embeddings(userInput)\n",
    "\n",
    "print(\"input shape: \", input_bert_embeddings.shape)\n",
    "# compute cosine simil\n",
    "\n",
    "# Lemmatized\n",
    "query_tfidf_lemmatized = tfidf_vectorizer_lemmatized.transform([lemmatized_query])\n",
    "cosine_scores_lemmatized = cosine_similarity(query_tfidf_lemmatized, X_tfidf_lemmatized)\n",
    "\n",
    "# stemmed\n",
    "query_tfidf_stemmed = tfidf_vectorizer_Stemmed.transform([stemmed_query])\n",
    "cosine_scores_stemmed = cosine_similarity(query_tfidf_stemmed, X_tfidf_Stemmed)\n",
    "\n",
    "# BERT Cosine Sim\n",
    "#input_bert_similarity = cosine_similarity(input_bert_embeddings.reshape(43,768))\n",
    "input_bert_similarity = cosine_similarity(input_bert_embeddings, bert_embeddings).flatten()\n",
    "top_indices = np.argsort(input_bert_similarity)[::-1][:5]\n",
    "\n",
    "#bert_similarity_list = [\n",
    "#    (i, j, input_bert_similarity[i, j])\n",
    "#    for i in range(input_bert_similarity.shape[0])\n",
    "#    for j in range(i + 1, input_bert_similarity.shape[1])\n",
    "#]\n",
    "\n",
    "# Sort based on similarity scores\n",
    "#sorted_bert_sim_list = sorted(bert_similarity_list, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for index in top_indices:\n",
    "    print(f\"Document {index} - {podcast_df['title'][index]} with similarity score: {input_bert_similarity[index]:.6f}\")\n",
    "    print(f\"Transcript: {corpus.iloc[index][19:150]}\")\n",
    "    print()\n",
    "print('--------------------------------------------')\n",
    "# Lemmatized\n",
    "top_n_documents(cosine_scores_lemmatized, 10, podcast_df)\n",
    "print('--------------------------------------------')\n",
    "# stemmed\n",
    "top_n_documents(cosine_scores_stemmed, 10, podcast_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f37a51be-0e5f-4adf-8d0f-6bb12ee8406f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2032385184.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Analyze and compare the results obtained from TF-IDF and BERT representations.\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Step 9: Compare Results\n",
    "\n",
    "#Analyze and compare the results obtained from TF-IDF and BERT representations.\n",
    "\n",
    "#Discuss the differences, strengths, and weaknesses of each method based on the retrieval results.\n",
    "\n",
    "Mediante la obtenci√≥n de los 10 resultados m√°s relevantes, mediante los modelos BERT y FT-IDF, con relaci√≥n al query ingresado, podemos darnos cuenta que ambos\n",
    "modelos funcionan muy bien ya que tanto el uno como el otro obtienen resultados relevantes a la query ingresada.\n",
    "\n",
    "    Considerando que la query es: \"technology computer innovation tomorrow new\"\n",
    "    y que los resultados para cada uno son:\n",
    "        BERT\n",
    "            Document 273 - Bitcoin, Inflation, and the Future of Money with similarity score: 0.757319\n",
    "            \n",
    "            Document 199 - Totalitarianism and Anarchy with similarity score: 0.754713\n",
    "            \n",
    "            Document 165 - Deep Work, Focus, Productivity, Email, and Social Media with similarity score: 0.744074\n",
    "            \n",
    "            Document 96 - Going Big in Business, Investing, and AI with similarity score: 0.737252\n",
    "            \n",
    "            Document 153 - Aliens, Black Holes, and the Mystery of the Oumuamua with similarity score: 0.734598\n",
    "\n",
    "        TF-IDF - Lemmatized\n",
    "            Top 1: Moore‚Äôs Law, Microprocessors, Abstractions, and First Principles - Similitud: 0.1185\n",
    "            \n",
    "            Top 2: Self-Driving Cars at Aurora, Google, CMU, and DARPA - Similitud: 0.0858\n",
    "            \n",
    "            Top 3: Qualcomm CEO - Similitud: 0.0857\n",
    "            \n",
    "            Top 4: Google - Similitud: 0.0801\n",
    "            \n",
    "            Top 5: Flying Cars, Autonomous Vehicles, and Education - Similitud: 0.0769\n",
    "            \n",
    "            Top 6: Affective Computing, Emotion, Privacy, and Health - Similitud: 0.0725\n",
    "            \n",
    "            Top 7: Aliens, Technology, Religion, and the Nature of Belief - Similitud: 0.0689\n",
    "            \n",
    "            Top 8: Computer Vision - Similitud: 0.0638\n",
    "            \n",
    "            Top 9: Quantum Computing - Similitud: 0.0606\n",
    "            \n",
    "            Top 10: Computer Architecture and Data Storage - Similitud: 0.0585\n",
    "\n",
    "        Tf-IDF - Stemmized\n",
    "            Top 1: Moore‚Äôs Law, Microprocessors, Abstractions, and First Principles - Similitud: 0.1702\n",
    "            \n",
    "            Top 2: Cellular Automata, Computation, and Physics - Similitud: 0.1137\n",
    "            \n",
    "            Top 3: Qualcomm CEO - Similitud: 0.1067\n",
    "            \n",
    "            Top 4: Simulation and Superintelligence - Similitud: 0.0968\n",
    "            \n",
    "            Top 5: Self-Driving Cars at Aurora, Google, CMU, and DARPA - Similitud: 0.0906\n",
    "            \n",
    "            Top 6: Robotics - Similitud: 0.0891\n",
    "            \n",
    "            Top 7: Flying Cars, Autonomous Vehicles, and Education - Similitud: 0.0875\n",
    "            \n",
    "            Top 8: Affective Computing, Emotion, Privacy, and Health - Similitud: 0.0863\n",
    "            \n",
    "            Top 9: Quantum Computing - Similitud: 0.0856\n",
    "            \n",
    "            Top 10: Google - Similitud: 0.0846\n",
    "\n",
    "    Entonces podemos observar que los resultados para el caso de TF-IDF son bastante parecidos, para el caso de BERT si existe una diferencia marcada en comparaci√≥n\n",
    "    a los resultados de TF-IDF. De igual forma los valores de similitud para TF-IDF son bajos en comparaci√≥n a los que se obtienen con BERT que son relativamente\n",
    "    altos y por tanto muestran una similitud m√°s alta.\n",
    "\n",
    "    En principio, hay que considera el diferente acercamiento a la solucion que cada modelo porporciona. Por una parte BERT es un modelo que considera m√°s el contexto\n",
    "    de un texto que TF-IDF el cual es un modelo m√°s bien estadistico. A diferencia de TF-IDF, BERT nos da un entendimiento m√°s profundo de la sint√°ctica y sem√°ntica\n",
    "    de las palabras u oraciones. Es por estas razones que los resultados de TF-IDF, tanto para el caso de lemmatizacion as√≠ como estamatizaci√≥n son muy semejantes ya\n",
    "    que la estadistica de las palabras es semejante en este modelo para todos los docuemntos del corpus. Por otro lado, BERT nos arroja resultados complemtamente\n",
    "    diferentes ya que el contexto interpretado del input, en comparaci√≥n con el propio contexto de los documentos se interpreta de una manera diferente a una mera\n",
    "    comparaci√≥n estad√≠stica. Incluso, este hecho es apoyado por los valores de similitud que arrojan los resultados, siendo estos valores mucho mayores en el caso\n",
    "    de los resultados de BERT a diferencia de los vores muy peque√±os de los resultados con TF-IDF tanto Lemmatizado como Estematizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4078a-dc49-4f59-96c5-8d931a8adb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
